# Guide Complet : Installer son IA locale avec Docker (Ollama + Open WebUI)

Ce guide vous permet de d√©ployer une solution d'Intelligence Artificielle 100% priv√©e, sans abonnement, fonctionnant enti√®rement sur votre propre mat√©riel. Nous allons coupler **Ollama** (le moteur) avec **Open WebUI** (l'interface graphique).

## üìã Pr√©requis

* **Docker** install√©s.
* **Syst√®me :** Linux, Windows (via WSL2) ou macOS.
* **Mat√©riel :** 8 Go de RAM minimum (16 Go recommand√©s). Un GPU NVIDIA est un plus majeur pour la rapidit√©.

---

## üõ† √âtape 1 : Pr√©paration du r√©seau et du stockage

Pour que les deux services communiquent de mani√®re stable avec des adresses IP fixes, nous cr√©ons un r√©seau Docker d√©di√©.

```bash
# Cr√©ation du r√©seau avec un sous-r√©seau sp√©cifique
docker network create --subnet=172.20.0.0/16 ai-network

# Cr√©ation du dossier pour stocker les mod√®les (persistance des donn√©es)
sudo mkdir -p /opt/ollama
sudo chmod 777 /opt/ollama

```

---

## üß† √âtape 2 : Installation du moteur Ollama

Ollama est le backend qui t√©l√©charge et ex√©cute les mod√®les (Llama 3, Mistral, Phi-3, etc.).

### Option A : Installation standard (CPU)

```bash
docker run -d \
  --network ai-network \
  --ip 172.20.0.30 \
  -v /opt/ollama:/root/.ollama \
  --name ollama \
  --restart always \
  ollama/ollama

```

### Option B : Installation optimis√©e (GPU NVIDIA)

*N√©cessite le [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html).*

```bash
docker run -d \
  --network ai-network \
  --ip 172.20.0.30 \
  --gpus all \
  -v /opt/ollama:/root/.ollama \
  --name ollama \
  --restart always \
  ollama/ollama

```

---

## üíª √âtape 3 : Installation de l'interface Open WebUI

Open WebUI offre une exp√©rience utilisateur fluide identique √† ChatGPT, avec gestion des documents (RAG), historique et multi-utilisateurs.

```bash
docker run -d \
  --network ai-network \
  --ip 172.20.1.100 \
  -p 3000:8080 \
  -e OLLAMA_BASE_URL=http://172.20.0.30:11434 \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:main

```

---

## üöÄ √âtape 4 : Premier d√©marrage et configuration

1. **Acc√®s :** Ouvrez votre navigateur sur `http://localhost:3000`.
2. **Compte Admin :** Cr√©ez votre compte. Le premier utilisateur devient l'administrateur syst√®me.
3. **Importation d'un mod√®le :**
   * Allez dans **Param√®tres** > **Mod√®les**.
   * Dans le champ "Pull a model from Ollama.com", entrez `llama3` ou `mistral`.
   * Cliquez sur l'ic√¥ne de t√©l√©chargement.
4. **Discutez :** Une fois le t√©l√©chargement fini, s√©lectionnez le mod√®le en haut de la page d'accueil et commencez √† discuter.

---

## üîç Pourquoi cette configuration ?

| Choix technique          | Avantage                                                                          |
| ------------------------ | --------------------------------------------------------------------------------- |
| **IP Statique**          | √âvite la perte de connexion entre l'interface et le moteur lors des red√©marrages. |
| **Volume `/opt/ollama`** | Vos mod√®les ne sont pas supprim√©s si vous supprimez le conteneur.                 |
| **R√©seau isol√©**         | S√©curise les flux de donn√©es entre vos conteneurs.                                |

---

## üí° Astuces de d√©pannage

* **V√©rifier les logs :** `docker logs -f ollama` pour voir si le moteur tourne correctement.
* **Tester la connexion :** Depuis la machine h√¥te, essayez d'acc√©der √† `http://172.20.0.30:11434`. Vous devriez voir le message *"Ollama is running"*.
* **Mise √† jour :** Pour mettre √† jour l'interface, faites simplement un `docker pull ghcr.io/open-webui/open-webui:main` et relancez le conteneur.

---

> **Note de s√©curit√© :** Par d√©faut, cette installation est accessible sur votre r√©seau local via l'IP de votre machine sur le port 3000. Pensez √† configurer un firewall si vous souhaitez limiter l'acc√®s.
